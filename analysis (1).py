# -*- coding: utf-8 -*-
"""analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RpwcMFwjsN9SZdyoqPuSbfNFYqKYpCRh
"""

from google.colab import drive
drive.mount('/content/drive/')

cd 'drive/MyDrive/paper_dissolved_oxygen_kosovo-master'

!pip install hydroeval

#!/usr/bin/python
# -*- coding: utf-8 -*-
import numpy as np
# import matplotlib
import pandas as pd
# from pandas.core.base import DataError
import math
import matplotlib.pyplot as pl
# import scipy as sp
import glob
import seaborn as sns
import re
import os
# import sys
# import itertools
import hydroeval as he
from scipy import stats
from sklearn.metrics import  (r2_score,
                              mean_squared_error,
                              mean_absolute_error)

# import skill_metrics as sm

#%%-----------------------------------------------------------------------------
pd.options.display.float_format = '{:.3f}'.format
palette_color="Set1"#"Blues_r"

def fmt(x):
    if (type(x) == str or type(x) == tuple or type(x) == list):
        return str(x)
    else:
      if (abs(x)>0.001 and abs(x)<1e0):
        return '%1.3f' % x
      else:
        return '%1.3f' % x #return '%1.3f' % x

def fstat(x):
  #m,s= '{:1.4g}'.format(np.mean(x)), '{:1.4g}'.format(np.std(x))
  #m,s, md= fmt(np.mean(x)), fmt(np.std(x)), fmt(np.median(x))
  m, s, md = np.mean(x), np.std(x), np.median(x)
  #text=str(m)+'$\pm$'+str(s)
  s = '--' if s<1e-8 else s
  text = fmt(m) + ' (' + fmt(s) + ')'#+' ['+str(md)+']'
  return text

def mean_percentual_error(y_true, y_pred):
  y_true, y_pred = np.array(y_true).ravel(), np.array(y_pred).ravel()
  return np.mean(np.abs(y_pred - y_true)/np.abs(y_true))*100

def VAF(y_true, y_pred):
  y_true, y_pred = np.array(y_true).ravel(), np.array(y_pred).ravel()
  return ( 1 - np.var(y_true - y_pred)/np.var(y_true) )*100

def accuracy_log(y_true, y_pred):
    y_true=np.abs(np.array(y_true))
    y_pred=np.abs(np.array(y_pred))
    return (np.abs(np.log10(y_true/y_pred))<0.3).sum()/len(y_true)*100


# http://www.jesshamrick.com/2016/04/13/reproducible-plots/
def set_style():
    # This sets reasonable defaults for size for
    # a figure that will go in a paper
    sns.set_context("paper")
    #pl.style.use(['seaborn-white', 'seaborn-paper'])
    #matplotlib.rc("font", family="Times New Roman")
    #(_palette("Greys", 1, 0.99, )
    #sns.set_palette("Blues_r", 1, 0.99, )
    sns.set_palette(palette_color, )
    sns.set_context("paper", font_scale=1.8,
                    rc = {"font.size"       :16,
                          "axes.titlesize"  :16,
                          "axes.labelsize"  :16,
                          'xtick.labelsize' :16,
                          'ytick.labelsize' :16,
                          'font.family'     :"Times New Roman",
                          }
        )
    # Set the font to be serif, rather than sans
    #sns.set(font='serif', font_scale=1.4,)

    # Make the background white, and specify the
    # specific font family
    sns.set_style(style="white",
                  rc={#"font.family": "serif",
                      "font.serif": ["Times"]
                      }
                  )

    sns.set_style("ticks")

    #os.system('rm -rf ~/.cache/matplotlib/tex.cache/')
    pl.rc('text', usetex=True)
    pl.rc('font',**{'family'    :'sans-serif',
                    'sans-serif':['Helvetica']
                    }
          )
    #pl.rc('font', family='sans-serif',  serif='Times')

#%%-----------------------------------------------------------------------------

# set_style()
# basename='join_data_basis'
# basename = 'newfoundland'
# basename = 'huainane_huaibei_coalfield'
basename = 'minmax_50_2_wq_kosovo'
run = 50
json_list = []
json_list += glob.glob('./json_minmax_50_2_wq_kosovo/*.json',recursive=True)
json_list.sort()

path = f'./images_{basename}/'

os.system('mkdir  '+path)
if not os.path.exists(path):
    os.makedirs(path)

#%%
#
# leitura dos dados
#
A=[]
for js in json_list:
    #print(pkl)
    df = pd.read_json(js)
    A.append(df)
#
A = pd.concat(A)#, sort=False)


#%%
models_to_remove = [
                    #'MARS',
                    #'XGB', 'RF', 'AB', 'RBFNN',
                    #'GPR',
                    #'SVR',
                    #'ANN'
                    #'GPR-FS', 'SVR-FS',
                    #'XGB-FS'
                    #'SVM', 'SVR-L', 'LSSVR'
                    'MLP',
                    'RNAAal','RNAAla','RNAAne','RNAAno',
                    ]
#models_to_remove = []
for m in models_to_remove:
    A = A[A['EST_NAME'] != m]


#datasets_to_remove = ['LDC case 8', 'LDC case 9',]
datasets_to_remove = []
#datasets_to_remove = ['LDC case '+str(i) for i in range(8)]
for m in datasets_to_remove:
    A = A[A['DATASET_NAME'] != m]

# Deixar comentadas as linhas abaixo
#if A['DATASET_NAME'].unique()[0] == 'Energy Efficiency':
#    A['DATASET_NAME'] = A['OUTPUT']; A['OUTPUT']='Load'
#A['DATASET_NAME'] = 'Irati'

A['DATASET_NAME'] = [x.split('-')[0] for x in A['DATASET_NAME']]
#%%
#steps=['TEST'] if 'Y_TEST_PRED' in A.columns else ['TRAIN']
steps=['TRAIN', 'TEST'] if 'Y_TEST_PRED' in A.columns else ['TRAIN']

C = []
for step in steps:
    for k in range(len(A)):
        df = A.iloc[k]
        y_true = pd.DataFrame(df['Y_'+step+'_TRUE'], columns=[df['OUTPUT']])#['0'])
        y_pred = pd.DataFrame(df['Y_'+step+'_PRED'], columns=[df['OUTPUT']])#['0'])
        #print (k, df['EST_PARAMS'])

        run = df['RUN']
        av = df['ACTIVE_VAR']
        ds_name = df['DATASET_NAME']
        s0 = ''.join([str(i) for i in av])
        s1 = ' '.join(['x_'+str(i) for i in av])
        s2 = '|'.join(['$x_'+str(i)+'$' for i in av])
        var_names = y_true.columns

        ds_name = df['DATASET_NAME']

        var_names = y_true.columns


        if len(y_true)>0:
            for v in var_names:
                _mape    = abs((y_true[v] - y_pred[v])/y_true[v]).mean()*100
                _vaf     = VAF(y_true[v], y_pred[v])
                _r2      = r2_score(y_true[v], y_pred[v])
                _mae     = mean_absolute_error(y_true[v], y_pred[v])
                _mse     = mean_squared_error(y_true[v], y_pred[v])
                _r       = stats.pearsonr(y_true[v], y_pred[v])[0]
                #_nse     = he.nse(y_true.values, y_pred.values)[0]

                #_lnse    = lognashsutcliffe(y_true.values, y_pred.values)
                _rmse    = he.rmse(y_true.values, y_pred.values)[0]
                #_rmsekx  = rmse_lower(y_true.values, y_pred.values, 'Kx')
                #_rmseq   = rmse_lower(y_true.values, y_pred.values, 'Q')
                _kge     = he.kge(y_true.values, y_pred.values)[0][0]
                _mare    = he.mare(y_true.values, y_pred.values)[0]
                dic     = {'Run'                    : run,
                           'Output'                 : v,
                           'MAPE'                   : _mape,
                           'R$^2$'                  : _r2,
                           'MSE'                    : _mse,

                           'Seed'                   : df['SEED'],
                           'Dataset'                : ds_name,
                           'Phase'                  : step,
                           'SI'                     : None,

                           'MARE'                   : _mare,
                           'MAE'                    : _mae,
                           'VAF'                    : _vaf,

                           # 'RMSELKX'                :rmsekx,
                           # 'RMSELQ'                 :rmseq,

                           'KGE'                    : _kge,
                           'RMSE'                   : _rmse,
                           'R'                      : _r,
                           'Internal Parameters'    : df['EST_PARAMS'],
                           'Parameters'             : df['OPT_PARAMS'],
                           'NDEI'                   :_rmse/np.std(y_true.values),

                           'y_true'                 :y_true.values.ravel(),
                           'y_pred'                 :y_pred.values.ravel(),
                           'Optimizer'              :df['ALGO'].split(':')[0], #A['ALGO'].iloc[0].split(':')[0],
                           #'Accuracy'                :accuracy_log(y_true.values.ravel(), y_pred.values.ravel()),
                           'Estimator'              :df['EST_NAME']
                           }
                C.append(dic)



metrics=[
        #'R',

        'R$^2$',
        #'RRMSE',
        #'RMSELKX',
        # 'RMSELQ',
        #'RMSE$(K_x<100)$',
        # 'RMSE$(B/H<50)$',
        'RMSE',
        #'NDEI',
        'MAE',
        #'Accuracy',
        #'MAPE',
        #'NSE',
        #'LNSE',
        #'KGE',
        'MARE',
        'MSE',
        #'VAF',
        # 'MAE (MJ/m$^2$)',
        # 'R',
        # 'RMSE (MJ/m$^2$)',
        ]

metrics_max =  ['NSE',
                'VAF',
                'R',
                'Accuracy',
                'R$^2$',
                'KGE',

                ]
#%%
anova=[]
for f, df in C.groupby(['Phase']):
    #df1 = df[df['Estimator']!=ref_estimator]
    df1 = df
    for (d, o,), df2 in df1.groupby(['Dataset', 'Output',]):
            if f[0]!='TRAIN':
                print('\n'+'='*80+'\n'+str(d)+' '+str(f)+' '+str(o)+'\n'+'='*80)
                nam = 'Estimator'
                groups = df2.groupby(nam,)
                print(df2['Estimator'].unique())

                for m in metrics:
                    #-pl.figure()
                    dic = {}
                    for g, dg in groups:
                        #-h=sns.distplot(dg[m].values, label=g)
                        dic[g] = dg[m].values

                    f_, p_ = stats.f_oneway(*dic.values())

                    #f_, p_ = stats.kruskal(*dic.values())
                    #-h.legend(); h.set_xlabel(m);
                    #-h.set_title('Dataset: '+d+'\n F-statistic = '+fmt(f_)+', '+'$p$-value = '+fmt(p_));
                    #-h.set_title('Dataset: '+d+' ($p = $'+fmt(p_)+')');
                    #-pl.ylabel(m)
                    #-pl.show()
                    anova.append({ #m:fmt(p_),
                                  'Phase'   : f,
                                  'Output'  : o,
                                  'Metric'  : m,
                                  'F-value' : fmt(f_),
                                  'p-value' : fmt(p_),
                                  'Dataset' : d})


anova = pd.DataFrame(anova)
groups = anova.groupby(['Dataset', 'Output'])
p_value_table = []
for g, dg in groups:
    dic = dict(zip(dg['Metric'], dg['p-value']))
    dic[' Dataset'] = g[0]
    dic[' Output'] = g[1]
    p_value_table.append(dic)

p_value_table = pd.DataFrame(p_value_table)

fn = basename + '_comparison_p_values_tukey_datasets' + '_table.tex'
fn = re.sub('-', '_', re.sub('\/', '', fn)).lower()
fn = path + fn
p_value_table = p_value_table.reindex(sorted(p_value_table.columns), axis=1)
p_value_table.to_latex(buf = fn, index = False)
print(p_value_table)
#%%
aux=[]
for (f,d,e,o,), df in C.groupby(['Phase', 'Dataset', 'Estimator','Output',]):
    #print(d,f,e,o,len(df))
    dic={}
    dic['Dataset']  = d
    dic['Phase']    = f
    dic['Output']   = o
    dic['Estimator']= e
    for f in metrics:
        dic[f] = fstat(df[f])

    aux.append(dic)

tbl = pd.DataFrame(aux)
tbl = tbl[tbl['Phase'] =='TEST']
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.options.display.width=140
df_table=[]
for (f,d,o), df in tbl.groupby(['Phase', 'Dataset','Output']):
#for (d,f), df in tbl.groupby(['Dataset','Phase', ]):
    for m in metrics:
        x, s = [], []
        for v in df[m].values:
            x_, s_ = v.split(' ')
            x_    = float(x_)
            x.append(x_)
            s.append(s_)

        x_idx     = np.argmax(x) if m in metrics_max else np.argmin(x)
        x         =[fmt(i) for i in x]
        x[x_idx]  = '{ \\bf '+x[x_idx]+'}'

        df[m]     = [ i+' '+j for (i,j) in zip(x,s)]


    fn = basename + '_comparison_datasets' + '_table_' + d.lower() + '_' + f.lower() + '.tex'
    fn = re.sub('\^', '', re.sub('\$', '', fn))
    fn = re.sub('\(', '', re.sub('\)', '', fn))
    fn = re.sub(' ', '_', re.sub('\/', '', fn))
    fn = path + fn

    print('\n' + '=' * 80 + '\n' + str(d) + ' ' + str(f) + '\n' + '=' * 80)
    print(fn)
    df['Modeling Phase'] = df['Phase']
    df.drop(['Phase',], axis = 1)
    #df1=df[['Modeling Phase', 'Dataset', 'Estimator', 'R', 'VAF', 'RMSE (MJ/m$^2$)', 'MAE (MJ/m$^2$)', 'NSE']]
    df1 = df[['Modeling Phase', 'Dataset', 'Output', 'Estimator', ]+metrics]
    #df.drop(['Output', 'Dataset', 'Phase'], axis=1, inplace=True)
    print(df1)
    df1.to_latex(buf = fn,
                 index = False,
                 escape = False,
                 label = fn,
                 caption = '',
                 column_format = 'r' * df1.shape[1])
    df_table.append(df1)

df_table = pd.concat(df_table)

cpt = 'Caption to be inserted.'
fn = basename + '_comparison_datasets' + '_table'
fn = re.sub('-', '_', re.sub('\/', '', fn)).lower()
fn = path + fn
df_table.drop(labels = ['Modeling Phase'], axis = 1, inplace=True)
#df_table.T.to_latex(buf=fn+'.tex', index=False, escape=False, label=fn, caption=cpt, column_format='r'*df_table.shape[1])
df_table.T.to_latex(buf = fn+'.tex',
                    index = True,
                    escape = False,
                    label = fn,
                    caption = cpt,
                    column_format = 'r' * df_table.shape[1])
print(df_table.T)

os.system('cp ' + fn + '.tex' + ' ./latex/tables/')

#%%
# radarchart
for (f,d,o,), df in C.groupby(['Phase', 'Dataset', 'Output',]):
    if f!='TEST':
        #print(df[metrics].columns)
        print(f,d,o)
        df_estimator=df.groupby(['Estimator'])[metrics].agg(np.mean)
        df_estimator.index=df_estimator.index.values

        categories=df_estimator.columns
        N = len(categories)

        for i in df_estimator.columns:
            if i in metrics_max:
                df_estimator[i]=df_estimator[i]/df_estimator[i].max()*100
            else:
                df_estimator[i]=df_estimator[i].min()/df_estimator[i]*100


        angles = [n / float(N) * 2 * math.pi for n in range(N)]
        angles += angles[:1]


        # initialise the spider plot
        pl.figure(figsize=(5,5), )#dpi=72)
        ax = pl.subplot(111, polar=True)

        # if you want the first axis to be on top:
        ax.set_theta_offset(math.pi / 2)
        ax.set_theta_direction(-1)

        # Draw one axe per variable + add labels labels yet
        pl.xticks(angles[:-1], categories, color='k')

        # Draw ylabels
        ax.set_rlabel_position(0)
        pl.ylim(0,108)
        ax.tick_params(axis='y', colors='grey',  grid_linestyle='--', size=7)
        ax.tick_params(axis='x', colors='grey',  grid_linestyle='--', size=7)

        for i in range(len(df_estimator)):
            values=df_estimator.iloc[i].values.flatten().tolist()
            values += values[:1]
            ax.plot(angles, values, linewidth=2, linestyle='solid', marker='o', label=df_estimator.index[i])
            ax.fill(angles, values,alpha=0.02)# 'w', alpha=0)

        # Add legend
        #pl.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
        #pl.title(d, y=1.1,)
        #pl.legend(loc=0, bbox_to_anchor=(1.12, 0.7), title=r"\textbf{"+d+"}", fancybox=True)
        pl.legend(loc=0, bbox_to_anchor=(1.12, 1.0), title=r"{"+d+' - '+o+"}", fancybox=True)

        fn = basename+'300dpi_radarchart_'+str(f)+'_'+str(d)+'_'+str(o)+'.png'
        fn = re.sub('\^','', re.sub('\$','',fn))
        fn = re.sub('\(','', re.sub('\)','',fn))
        fn = re.sub(' ','_', re.sub('\/','',fn))
        fn = re.sub('-','_', re.sub('\/','',fn)).lower()
        fn = path + fn
        pl.savefig(fn,  bbox_inches='tight', dpi=300)

        pl.show()

#sys.exit()
#%%

for(d,df) in C.groupby('Dataset'):
     # https://github.com/pog87/PtitPrince/blob/master/RainCloud_Plot.ipynb
    n_estimators = df['Estimator'].unique().shape[0]
    ds=df['Output'].unique(); ds.sort()
    hs=df['Estimator'].unique(); hs.sort(); #hs=np.concatenate([hs[hs!=ref_estimator],hs[hs==ref_estimator]])
    #sns.set(font_scale=2.5)
    for kind in ['box',]:#'box', 'violin']:
        for m in metrics:
            kwargs={'edgecolor':"k", 'capsize':0.05, 'alpha':0.95, 'ci':'sd', 'errwidth':1.0, 'dodge':True, 'aspect':1, 'legend':None, } if kind=='bar' else {'notch':1, 'ci':'sd','aspect':0.618,}
    #        sns.catplot(x='Dataset', y='R$^2$', data=C, hue='Phase', kind='bar', col='Estimator')
    #        g=sns.catplot(x='Dataset', y=m, col='Estimator', data=C,
    #                       kind=kind, sharey=False, hue='Phase',
    #                       **kwargs,);
    #        g=sns.catplot(col='Dataset', y=m, hue='Estimator', data=C,
    #                       kind=kind, sharey=False, x='Phase',
    #                       **kwargs,);
            if kind=='bar':
                g=sns.catplot(x='Estimator', y=m, #hue='Estimator', #row='Phase',
                              data=df[df['Phase']!='TEST'],
                              order=ds, hue_order=hs, kind=kind, sharey=False,
                              #col_wrap=2, palette=palette_color_1,
                              **kwargs,)
            elif kind=='box':
                g=sns.catplot(col='Output', y=m, #hue='Estimator', x='Phase',
                              x='Estimator',
                              #order=ds, hue_order=hs,
                             data=df[df['Phase']=='TEST'],  legend=False,
                              kind=kind, sharey=False, #col_wrap=2,
                              **kwargs,)
                for ax in g.axes.ravel():
                    ax.set_xticklabels(labels=ax.get_xticklabels(),rotation=90,)# fontdict={'fontsize':17})

            elif kind=='violin':
                g=sns.catplot(x='Output', y=m, hue='Estimator', col='Phase',
                              data=df[df['Phase']!='TEST'],
                              #order=ds, hue_order=hs,
                              scale="count", #inner="quartile",
                              count=0, legend=False,
                              kind=kind, sharey=False,  #col_wrap=2,
                              **kwargs,)
            else:
                pass

            #g.despine(left=True)
            fmtx='%2.3f'
            for ax in g.axes.ravel():
                #ax.set_xticklabels(labels=ax.get_xticklabels(),rotation=00,)# fontdict={'fontsize':17})
                if kind=='bar':
                    ax.set_ylim([0, 1.15*ax.get_ylim()[1]])
                    ax.set_xlabel(None); #ax.set_ylabel(m);
                    _h=[]
                    for p in ax.patches:
                        _h.append(p.get_height())

                    _h=np.array(_h)
                    _h=_h[~np.isnan(_h)]
                    _h_max = np.max(_h)
                    for p in ax.patches:
                        _h= 0 if np.isnan(p.get_height()) else p.get_height()
                        p.set_height(_h)
                        ax.text(
                                x=p.get_x() + p.get_width()/2.,
                                #y=1.04*p.get_height(),
                                y=0.02*_h_max+p.get_height(),
                                s=fmtx % p.get_height() if p.get_height()>0 else None,
                                #fontsize=16,
                                color='black', ha='center',
                                va='bottom', rotation=90, weight='bold',
                                )
                #pl.legend(bbox_to_anchor=(1.20, 0.5), loc=10, borderaxespad=0., ncol=1, fontsize=14, title=r"{"+d+' - '+o+"}", fancybox=True )
                #pl.legend(loc=0, ncol=n_estimators, borderaxespad=0.,)

            fn = basename+'300dpi_comparison_'+d+'_metric_'+m.lower()+'_'+kind+'.png'
            fn = re.sub('\^','', re.sub('\$','',fn))
            fn = re.sub('\(','', re.sub('\)','',fn))
            fn = re.sub(' ','_', re.sub('\/','',fn))
            fn = re.sub('-','_', re.sub('\<','_',fn)).lower()
            fn = path + fn
            print(fn)
            pl.savefig(fn,  bbox_inches='tight', dpi=300)

            pl.show()

#sys.exit()
#%%
def replace_names(s):
    sv = [
            ('gamma', '$\gamma$'), ('epsilon','$\\varepsilon$'), ('C', '$C$'),
            ('l1_ratio','$L_1$ ratio'), ('alpha','$\\alpha$'),
            ('l2_penalty','$C_2$'),
            ('thin_plate','T. Plate'),('cubic','Cubic'),
            ('inverse','Inverse'),('quintic','Quintic'),('linear','Linear'),
            ('penalty','$\gamma$'),('max_degree','$q$'),
            ('hidden_layer_sizes', 'HL'),
            ('learning_rate_init', 'LR'),
            ('rbf_width', '$\gamma$'),
            ('activation_func', '$G$'),
            ('activation', '$\\varphi$'),
            ('n_hidden', 'HL'),
            ('sigmoid', 'Sigmoid'),
            ('inv_multiquadric', 'Inv. Multiquadric'),
            ('multiquadric', 'Multiquadric'),
            ('hardlim', 'HardLim'),('softlim', 'SoftLim'),
            ('tanh', 'Hyp. Tangent'),
            ('gaussian', 'Gaussian'),
            ('identity', 'Identity'),
            ('swish', 'Swish'),
            ('relu', 'ReLU'),
            ('Kappa', '$\kappa$'),
            ('criterion','Criterion'),
            ('learning_rate','LR'),
            ('friedman_mse','MSE'),
            ('reg_lambda','$\lambda$'),
            ('max_depth','Max. Depth'),
            ('min_samples_leaf','Min. Samples Leaf'),
            ('min_samples_split','Min. Samples Split'),
            ('min_weight_fraction_leaf', 'Min. Weig. Fract. Leaf'),
            ('n_estimators', 'No Estimators'),
            ('presort', 'Presort'),
            ('subsample', 'Subsample'),
            ('n_neighbors','$K$'),
            ('positive','Positive Weights'),
            ('max_terms','Max. Terms'),
            ('max_iter','Max. Iter.'),
            ('min_child_weight','Min. Child Weight'),
            ('colsample_bytree','Col. Sample'),
            ('thin_plate', 'thin-plate'),
            ('interaction_only','Interaction Only'),
            ('k1','$k_0$'),
            ('sigma', '$\sigma$'), ('beta', '$\\beta$'),
            ('U/u*','$U/u^*$'),
            ('B','$B$'),('H','$H$'),('U','$U$'),('u*','$u^*$'),
        ]
    for s1,s2 in sv:
        r=s.replace(str(s1), s2)
        if(r!=s):
            #print r
            return r
    return r
#%%
parameters=pd.DataFrame()
for (p,d,e,o), df in C.groupby(['Phase','Dataset','Estimator','Output']):
  #if e!= ref_estimator:
    print (p+'\t'+d+'\t\t'+e+'\t'+str(len(df)))
    aux={}
    par = pd.DataFrame(list(df['Parameters']))
    par = pd.DataFrame(list(df['Internal Parameters']))
    #print(e,par.values)
    if 'random_state' in par.columns:
        par.drop(['random_state',], axis=1, inplace=True)

    if e=='RR':
        par.drop(['copy_X','fit_intercept','max_iter','normalize','solver','solver',], axis=1, inplace=True)

    if e=='RBFNN':
        #par['hidden_layer_sizes']=[len(j) for j in par['hidden_layer_sizes']]
        _t=['func',]
        for t in _t:
            par[t] = [replace_names(i) for i in par[t].values]

        #print(par); print('\n\n\n\n\n')

    if e=='ANN':
        par['hidden_layer_sizes']=[len(j) for j in par['hidden_layer_sizes']]
        _t=['activation',]
        for t in _t:
            par[t] = [replace_names(i) for i in par[t].values]

    if  e=='ELM':
        par.drop(['activation_args','user_components',], axis=1, inplace=True)
        par.drop(['regressor'], axis=1, inplace=True)
        _t=['activation_func',]
        for t in _t:
            par[t] = [replace_names(i) for i in par[t].values]

        par1 = par.rename({'activation_func':'$G$', 'n_hidden':'HL'}, axis=1)
        sns.catplot(data=par1, y='$G$', x='HL', kind='box', aspect=0.6); pl.ylabel('')
        #sys.exit()

    if  e=='XGB':
        #par.drop(['objective'], axis=1, inplace=True)
        print(par)

    if  e=='SVR' or e=='SVR-FS':
        par.drop(['max_iter','shrinking','tol', 'verbose', 'degree','cache_size', 'coef0'], axis=1, inplace=True)
        #par['gamma'] = [0 if a=='scale' else a for a in par['gamma']]
        print(par)
        #sys.exit()

    if  e=='GPR' or e=='GPR-FS':
        par['$\\nu$'] = [float(str(a).split('nu=')[1].split(')')[0]) for a in par['kernel']]
        par['$l$'] = [float(str(a).split('length_scale=')[1].split(', nu')[0]) for a in par['kernel']]
        par.drop(labels=['kernel'], axis=1, inplace=True)
        print(par)
        #sys.exit()

    par=par.melt()
    par['Estimator']=e
    par['Dataset']=d
    par['Phase']=p
    par['Output']=o
    par['variable'] = [replace_names(i) for i in par['variable'].values]

    parameters = pd.concat([parameters,par], sort=True)

parameters['Parameter']=parameters['variable']
#parameters=parameters[parameters['Parameter']!='regressor']

#%%
for (p,e,t,o), df in parameters.groupby(['Phase','Estimator', 'Parameter','Output']):
 if p!='TEST':
  #if e!= ref_estimator:
   #if '-FS' in e:
    print ('='*80+'\n'+t+' - '+e+' - '+str(o)+'\n'+'='*80+'\n')
    pl.figure()
    if df['value'].unique().shape[0]<= 10:
        #df['value']=df['value'].astype(int,errors='ignore',)
        kwargs={"linewidth": 1, 'edgecolor':None,}
        try:
            g = sns.catplot(x='value',
                            col='Dataset',
                            kind='count',
                            data=df,
                            col_wrap=4,
                            aspect=0.618,
                            palette=palette_color,
                            **kwargs)
            fmtx='%3d'
            g.set_ylabels('Frequency')#(e+': Parameter '+t)
            g.fig.tight_layout()

            for ax in g.axes.ravel():
                ax.axes.set_xlabel(e+': Parameter '+t)
                ax.set_xticklabels(labels=ax.get_xticklabels(),rotation=90, fontsize=10,)
                ax.set_ylim([0, 1.05*ax.get_ylim()[1]])
                ylabels = ['%3d'% x for x  in ax.get_yticks()]
                ax.set_yticklabels(ylabels, fontsize=10,)
                ax.set_xlabel(e+': '+t,fontsize=10,)

                #ax.set_xlabel('Day'); #ax.set_ylabel(m);

            for ax in g.axes.ravel():
                _h=[]
                for pat in ax.patches:
                    _h.append(pat.get_height())

                _h=np.array(_h)
                _h=_h[~np.isnan(_h)]
                _h_max = np.max(_h)
                for pat in ax.patches:
                    _h= 0 if np.isnan(pat.get_height()) else pat.get_height()
                    pat.set_height(_h)
                    ax.text(
                            x=pat.get_x() + pat.get_width()/2.,
                            #y=1.04*p.get_height(),
                            y=0.05*_h_max+pat.get_height(),
                            s=fmtx % pat.get_height(),
                            fontsize=10,
                            color='black', ha='center',
                            va='bottom', rotation=0, weight='bold',
                           )
            #pl.legend( loc=10, borderaxespad=0., fontsize=16, )
            #pl.show()
        except:
            pass
    else:
        df['value']=df['value'].astype(float,errors='ignore',)
        kwargs={"linewidth": 1, 'aspect':0.518,}
        g = sns.catplot(y='value', x='Dataset', kind='box', data=df, notch=1,
                        orient='v', palette=palette_color, **kwargs,)
        #xmin, xmax = g.ax.get_xlim()
        #g.ax.set_xlim(left=0, right=xmax)
        #g.ax.set_xlabel(d+' -- '+e+': Parameter '+t, fontsize=16,)
        g.ax.set_xlabel(e+': Parameter '+t, )#fontsize=16,)
        g.ax.set_ylabel(d, rotation=90)
        g.ax.set_ylabel(None)#fontsize=16,)
        g.fig.tight_layout()
        #g.fig.set_figheight(4.00)
        #pl.xticks(rotation=45)
        #g.ax.set_ylabel(e+': Parameter '+t)
        #sys.exit()

#    min, xmax = g.ax.get_xlim()
#    g.ax.set_xlim(left=0, right=xmax)
#    g.fig.tight_layout()
#    g.fig.set_figheight(0.50)
#    pl.xticks(rotation=45)
    fn = basename+'300dpi_comparison_datasets'+'_parameters_'+'__'+e+'__'+t+'__'+p+'.png'
    fn = re.sub('\^','', re.sub('\$','',fn))
    fn = re.sub('\(','', re.sub('\)','',fn))
    fn = re.sub(' ','_', re.sub('\/','',fn))
    fn = re.sub('\\\\','', re.sub('x.','x',fn))
    fn = re.sub('-','_', re.sub('\/','',fn)).lower()
    fn = fn.lower()
    fn = path + fn
    print(fn)
    pl.savefig(fn, transparent=True, dpi=300)
    pl.show()

#sys.exit()
#%%


from sklearn.ensemble import GradientBoostingRegressor
from lightgbm import LGBMRegressor
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.svm import SVR
from sklearn.linear_model import ElasticNet


# from read_data_v0p2 import read_data
# from read_data_huainane import read_data
# from read_data_oriente import read_data
# from sklearn.gaussian_process import GaussianProcessRegressor
# from sklearn.preprocessing import MaxAbsScaler

package = f"kosovo_read_data"
name = "read_kosovo"

read_data = getattr(__import__(package, fromlist=[name]), name)

dataset=read_data()
# set_style()

feature_names    = dataset['feature_names'   ]
X_train          = dataset['X_train'         ]
X_test           = dataset['X_test'          ]
y_train          = dataset['y_train'         ]
y_test           = dataset['y_test'          ]
n_features       = dataset['n_features'      ]


v_ref = 'RMSE'
v_aux = 'R$^2$'
k = -1
unc_tab=[]
for (e,d,o,p), df in C.groupby(['Estimator','Dataset','Output','Phase']):
 if p!='TRAIN':
  #if e!= ref_estimator:
   #if '-FS' in e:
    #print ('='*80+'\n'+p+' - '+d+' - '+e+' - '+str(o)+'\n'+'='*80+'\n')

    k = df[v_ref].idxmin()
    aux = df.loc[k]
    par   = aux['Parameters'].copy()
    param = aux['Internal Parameters'].copy()

    X_train_ = pd.DataFrame(data=X_train, columns=feature_names)
    X_test_  = pd.DataFrame(data=X_test, columns=feature_names)
    if len(X_test_)==0:
        X_test_ = X_train_.copy()

    #if e=='ELM':
    #    _alpha = param['l2_penalty']
    #    param.pop('l2_penalty')
    #    regressor = None if _alpha<1e-4 else Ridge(alpha=_alpha,random_state=aux['Seed'])
    #    param['regressor']=regressor

    estimators={
        'SVR'   :SVR(kernel='rbf',max_iter=500),
        'EN'    :ElasticNet(),
        'LGBM':LGBMRegressor()
        }

    #active_features = aux['Active Variables']
    #active_features = [s.replace(' ','') for s in active_features.split(',')]

    X_train_ = X_train_[feature_names].values
    X_test_  = X_test_[feature_names].values
    n_features = X_train_.shape[1]

    #scaler=MaxAbsScaler()
    #scaler.fit(X_train_)
    #X_train_ = scaler.transform(X_train_)
    #X_test_  = scaler.transform(X_test_)

    #reg = SVR() if 'SVR' in e else GaussianProcessRegressor(optimizer=None)
    reg=estimators[e]

    for pr in ['scaler', 'k1']:
        if pr in param.keys():
            param.pop(pr)

    reg.set_params(**param)
    reg.fit(X_train_, y_train.reshape(-1,1))



    n_outcomes=250000
    data=np.random.uniform( low=X_test_.min(axis=0), high=X_test_.max(axis=0), size=(n_outcomes, X_test_.shape[1]) )
    #data=np.random.normal( loc=X_test_.mean(axis=0), scale=X_test_.std(axis=0), size=(n_outcomes, X_test_.shape[1]) )
    predict = reg.predict(data)
    median = np.median(predict)
    mad=np.abs(predict - median).mean()
    uncertainty = 100*mad/median
    print(e,d, median, mad, n_features, uncertainty/n_features, uncertainty)
    dc={'Model':e, 'Case':d, 'No. features':n_features, 'Median':median,
        'MAD':mad, 'Uncertainty':uncertainty, v_ref:aux[v_ref]}
    unc_tab.append(dc)

    #
    #
    #
    #import uncertainpy as un
    #import numpy as np                   # For the time array
    #import chaospy as cp

#%%
unc_tab = pd.DataFrame(unc_tab)
fn='uncertainty_table__mc'
fn = path + fn
cpt='Caption to be inserted.'
# fn = path + fn
unc_tab.to_latex(buf=fn+'.tex', index=False, escape=False, label=fn, caption=cpt, column_format='r'*df_table.shape[1], float_format="%.1f")
print(unc_tab)

# unc_tab=unc_tab[unc_tab['Case']!='Case 1']
unc_tab.index=[i for i in range(unc_tab.shape[0])]

#unc_tab['case']=['FS' if 'FS' in t else 'C'+s.split(' ')[1] for s,t in zip(unc_tab['Case'],unc_tab['Model'])]
# #unc_tab['case']=['FS' if 'FS' in t else s for s,t in zip(unc_tab['Case'],unc_tab['Model'])]

pl.figure(figsize=(4,4))
p1=sns.relplot(x='Uncertainty', y=v_ref, hue='Model', size='No. features',
                #xstyle='Model',
                sizes=(600, 600),size_norm=(1,len(unc_tab['No. features'].unique())),
                data=unc_tab, alpha=0.7,
                )
# for line in range(0,unc_tab.shape[0]):
#       p1.ax.text(x=unc_tab['Uncertainty'][line]+0, y=unc_tab[v_ref][line],
#               s=fmt(unc_tab['MAD'][line]),
#               horizontalalignment='left', size='large', color='black',
#               weight='semibold', rotation=0)

fn = basename+'300dpi_comparison_uncertainty_rmse'+'.png'
fn = re.sub('\^','', re.sub('\$','',fn))
fn = re.sub('\(','', re.sub('\)','',fn))
fn = re.sub(' ','_', re.sub('\/','',fn))
fn = re.sub('\\\\','', re.sub('x.','x',fn))
fn = re.sub('-','_', re.sub('\/','',fn)).lower()
fn = fn.lower()
fn = path + fn
#print(fn)
pl.savefig(fn, transparent=True,
            bbox_inches='tight',
            dpi=300)
pl.show()
#%%
v_ref = 'RMSE'
v_aux = 'R$^2$'
k = -1
unc_tab=[]
for (e,d,o,p), df in C.groupby(['Estimator','Dataset','Output','Phase']):
  if p!='TRAIN':
  #if e!= ref_estimator:
    #if '-FS' in e:
    #print ('='*80+'\n'+p+' - '+d+' - '+e+' - '+str(o)+'\n'+'='*80+'\n')

    k = df[v_ref].idxmin()
    aux = df.loc[k]

    ix=aux['y_pred']>0; unc_p,unc_t = aux['y_pred'][ix], aux['y_true'][ix]
    unc_e = (np.log10(unc_p) - np.log10(unc_t))
    unc_m=unc_e.mean()
    unc_s = np.sqrt(sum((unc_e - unc_m)**2)/(len(unc_e)-1))
    pei95=fmt(10**(-unc_m-1.96*unc_s))+' to '+fmt(10**(-unc_m+1.96*unc_s))
    #print(p+' - '+d+' - '+e+' - '+str(o), fmt(unc_m), fmt(unc_s), pei95 )
    sig = '+' if unc_m > 0 else ''
    dc={'Model':e, 'Case':d, 'MPE':sig+fmt(unc_m), 'WUB':'$\pm$'+fmt(unc_s), 'PEI95':pei95}
    unc_tab.append(dc)


unc_tab = pd.DataFrame(unc_tab)
fn='uncertainty_table__models'
fn = path + fn
cpt='Caption to be inserted.'
unc_tab.to_latex(buf=fn+'.tex', index=False, escape=False, label=fn, caption=cpt, column_format='r'*df_table.shape[1])
print(unc_tab)
#%%
stations = C['Dataset'].unique()
stations.sort()
colors={}
for i, j in zip(stations,['r', 'darkgreen', 'b', 'm', 'c','y', 'olive',  'darkorange', 'brown', 'darkslategray', ]):
    colors[i]=j

for i, j in zip(C['Estimator'].unique(),['r', 'darkgreen', 'b', 'm', 'c','y', 'olive',  'darkorange', 'brown', 'darkslategray', ]):
    colors[i]=j

v_ref = 'RMSE'
v_aux = 'R$^2$'
k = -1
for (e,d,o,p), df in C.groupby(['Estimator','Dataset','Output','Phase']):
  if p=='TEST':
  #if e!= ref_estimator:
    #if '-FS' in e:
    print ('='*80+'\n'+p+' - '+d+' - '+e+' - '+str(o)+'\n'+'='*80+'\n')

    k = df[v_ref].idxmin()
    aux = df.loc[k]
    pl.figure(figsize=(4,4))
    ax = sns.regplot(x="y_true", y="y_pred", data=aux, ci=0.95,
                      line_kws={'color':'black'},
                      scatter_kws={'alpha':0.85, 'color':colors[e], 's':100},
                      #label='WI'+' = '+fmt(aux['WI']),
                      #label='R'+' = '+fmt(aux['R']),
                      label='R$^2$'+' = '+fmt(aux['R$^2$']),
                      )
    #ax.set_xscale("log"); ax.set_yscale("log")
    ax.set_xlim(left    = aux['y_true'].min(), right    = 1.1*aux['y_true'].max() )
    ax.set_ylim(bottom  = aux['y_true'].min(), top      = 1.1*aux['y_true'].max() )
    ax.set_title(d+' -- '+e+' ({\\bf '+p+'}) '+'\n'+v_ref+' = '+fmt(df[v_ref][k]))
    ax.set_title(d+' -- '+e+' ('+p+') '+'\n'+v_ref+' = '+fmt(df[v_ref][k])+', '+v_aux+' = '+fmt(df[v_aux][k]))
    ax.set_xlabel('Measured   '+aux['Output'])
    ax.set_ylabel('Predicted  '+aux['Output'])
    ylabels = ['%1.3f'% x for x  in ax.get_yticks()]
    xlabels = ['%1.3f'% x for x  in ax.get_xticks()]
    ax.set_yticklabels(labels=ylabels, rotation=0)
    ax.set_xticklabels(labels=xlabels, rotation=0)
    ax.set_aspect(1)
    ax.legend(frameon=False, markerscale=0, loc=0)
    fn = basename+'300dpi_scatter'+'_best_model_'+'__'+e+'__'+d+'__'+p+'.png'
    fn = re.sub('\^','', re.sub('\$','',fn))
    fn = re.sub('\(','', re.sub('\)','',fn))
    fn = re.sub(' ','_', re.sub('\/','',fn))
    fn = re.sub('\\\\','', re.sub('x.','x',fn))
    fn = re.sub('-','_', re.sub('\/','',fn)).lower()
    fn = fn.lower()
    fn = path + fn
    #print(fn)
    pl.savefig(fn, transparent=True, bbox_inches='tight', dpi=300)

    pl.show()

dic

for f, df in C.groupby(['Phase']):
    #df1 = df[df['Estimator']!=ref_estimator]
    df1 = df
    for (d, o,), df2 in df1.groupby(['Dataset', 'Output',]):
            if f[0]=='TRAIN':
                print('\n'+'='*80+'\n'+str(d)+' '+str(f)+' '+str(o)+'\n'+'='*80)
                nam = 'Estimator'
                groups = df2.groupby(nam,)
                print(df2['Estimator'].unique())

                for m in metrics:
                    #-pl.figure()
                    dic = {}
                    for g, dg in groups:
                        #-h=sns.distplot(dg[m].values, label=g)
                        dic[g] = dg[m].values

                    f_= stats.tukey_hsd(*dic.values())
                    print(m)
                    print(f_)


